---
title: "Data processing & ETL workflows"
sidebarTitle: "Data processing & ETL"
description: "Learn how to use Trigger.dev for data processing and ETL including web scraping, database synchronization, batch enrichment, and streaming analytics workflows"
---

import UseCasesCards from "/snippets/use-cases-cards.mdx";

## Overview

Build data pipelines that process large datasets without timeouts. Handle streaming analytics, batch enrichment, web scraping, database sync, and file processing with automatic retries and progress tracking.

## Featured examples

<CardGroup cols={3}>
  <Card
    title="Realtime CSV importer"
    icon="book"
    href="/guides/example-projects/realtime-csv-importer"
  >
    Import CSV files with progress streamed live to frontend.
  </Card>
  <Card title="Web scraper with BrowserBase" icon="book" href="/guides/examples/scrape-hacker-news">
    Scrape websites using BrowserBase and Puppeteer.
  </Card>
  <Card
    title="Supabase database operations"
    icon="book"
    href="/guides/examples/supabase-database-operations"
  >
    Run CRUD operations on Supabase database tables.
  </Card>
</CardGroup>

## Why Trigger.dev for data processing

**Process datasets for hours without timeouts**

Handle multi-hour transformations, large file processing, or complete database exports. No execution time limits.

**Parallel processing with built-in rate limiting**

Process thousands of records simultaneously while respecting API rate limits. Scale efficiently without overwhelming downstream services.

**Stream progress to your users in real-time**

Show row-by-row processing status updating live in your dashboard. Users see exactly where processing is and how long remains.

## Common workflows

Here are some basic examples of data processing and ETL workflows:

<Tabs>
  <Tab title="ETL pipeline">
    <Steps>
      <Step title="Extract">Pull from APIs, databases, S3, or web scraping</Step>
      <Step title="Transform">Clean, validate, enrich data</Step>
      <Step title="Load">Write to warehouse, database, or storage</Step>
      <Step title="Monitor">Track progress, handle failures</Step>
    </Steps>
  </Tab>
  <Tab title="Web scraping">
    <Steps>
      <Step title="Navigate">Load target pages with headless browser</Step>
      <Step title="Extract">Pull content, links, structured data</Step>
      <Step title="Transform">Clean HTML, parse JSON, normalize data</Step>
      <Step title="Store">Save to database or file storage</Step>
    </Steps>
  </Tab>
  <Tab title="Batch enrichment">
    <Steps>
      <Step title="Query">Fetch records needing enrichment</Step>
      <Step title="Enrich">Call external APIs in parallel batches</Step>
      <Step title="Validate">Check data quality and completeness</Step>
      <Step title="Update">Write enriched data back to database</Step>
    </Steps>
  </Tab>
  <Tab title="File processing">
    <Steps>
      <Step title="Upload">Receive file via webhook or storage event</Step>
      <Step title="Parse">Read CSV, JSON, XML, or binary format</Step>
      <Step title="Process">Transform, validate, chunk large files</Step>
      <Step title="Import">Bulk insert to database or data warehouse</Step>
    </Steps>
  </Tab>
</Tabs>

<UseCasesCards />
