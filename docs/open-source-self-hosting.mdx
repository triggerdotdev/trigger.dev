---
title: "Self-hosting"
description: "You can self-host Trigger.dev on your own infrastructure."
---

## Overview

<Frame>
  <img src="/images/self-hosting.png" alt="Self-hosting architecture" />
</Frame>

The self-hosting guide comes in two parts. The first part is a simple setup where you run everything on one server. In the second part, the webapp and worker components are split on two separate machines.

You're going to need at least one Debian (or derivative) machine with Docker and Docker Compose installed. We'll also use Ngrok to expose the webapp to the internet.

## Caveats

<Note>The v3 worker components don't have ARM support yet.</Note>

This guide outlines a quick way to start self-hosting Trigger.dev. Scaling, security, and reliability concerns are not fully addressed here. It's unlikely to result in a production-ready deployment on its own, but it's a good starting point.

As self-hosted deployments tend to have unique requirements and configurations, we don't provide specific advice for scaling up or improving security and reliability.

Should the burden ever get too much, we'd be happy to see you on [Trigger.dev cloud](https://trigger.dev/pricing) where we deal with these concerns for you.

<Accordion title="Please consider these additional warnings">
  - The Docker [checkpoint command](https://docs.docker.com/reference/cli/docker/checkpoint/) is an
  experimental feature which may not work as expected. It won't be enabled by default. Instead, the
  containers will stay up and their processes frozen. They won't consume CPU but they _will_ consume
  RAM. - The Docker provider does not currently enforce any resource limits. This means your tasks
  can consume up to the total machine CPU and RAM. Having no limits may be preferable when
  self-hosting, but can impact the performance of other services. - The worker components (not the
  tasks!) have direct access to the Docker socket. This means they can run any Docker command. To
  restrict access, you may want to consider using [Docker Socket
  Proxy](https://github.com/Tecnativa/docker-socket-proxy). - The task containers are running with
  host networking. This means there is no network isolation between them and the host machine. They
  will be able to access any networked service on the host. - There is currently no support for
  adding multiple worker machines. This would require a more elaborate provider, or possibly a
  switch to Docker Swarm. This is not currently planned, but you are welcome to
  [contribute](https://github.com/triggerdotdev/trigger.dev/blob/main/CONTRIBUTING.md).
</Accordion>

## Requirements

- 4 CPU
- 8 GB RAM
- Debian or derivative
- Optional: A separate machine for the worker components

You will also need a way to expose the webapp to the internet. This can be done with a reverse proxy, or with a service like Ngrok. We will be using the latter in this guide.

## Part 1: Single server

This is the simplest setup. You run everything on one server. It's a good option if you have spare capacity on an existing machine, and have no need to independently scale worker capacity.

### Server setup

Some very basic steps to get started:

1. [Install Docker](https://docs.docker.com/get-docker/)
2. [Install Docker Compose](https://docs.docker.com/compose/install/)
3. [Install Ngrok](https://ngrok.com/download)

On a Debian server, you can install everything you need with the following commands:

```bash
curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | \
    sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && \
    echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | \
    sudo tee /etc/apt/sources.list.d/ngrok.list

sudo apt-get update
sudo apt-get install -y \
    docker.io \
    docker-compose \
    ngrok
```

### Trigger.dev setup

1. Clone the [Trigger.dev docker repository](https://github.com/triggerdotdev/docker) and checkout the v3 branch

```bash
git clone https://github.com/triggerdotdev/docker
cd docker
git checkout v3
```

2. Run the start script and follow the prompts

```bash
./start.sh # hint: you can append -d to run in detached mode
```

### Manual setup

Alternatively, you can follow these manual steps after cloning the docker repo:

1. Create the `.env` file

```bash
cp .env.example .env
```

2. Generate the required secrets

```bash
echo MAGIC_LINK_SECRET=$(openssl rand -hex 16)
echo SESSION_SECRET=$(openssl rand -hex 16)
echo ENCRYPTION_KEY=$(openssl rand -hex 16)
echo PROVIDER_SECRET=$(openssl rand -hex 32)
echo COORDINATOR_SECRET=$(openssl rand -hex 32)
```

3. Replace the default secrets in the `.env` file with the generated ones

4. Run docker compose to start the services

```bash
. lib.sh # source the helper function
docker_compose -p=trigger up
```

### Tunnelling

You will need to expose the webapp to the internet. You can use Ngrok for this. If you already have a working reverse proxy setup and a domain, you can skip to the last step.

1. Start Ngrok. You may get prompted to sign up - it's free.

```bash
./tunnel.sh
```

2. Copy the domain from the output, for example: `1234-42-42-42-42.ngrok-free.app`

3. Uncomment the `TRIGGER_PROTOCOL` and `TRIGGER_DOMAIN` lines in the `.env` file. Set it to the domain you copied.

```bash
TRIGGER_PROTOCOL=https
TRIGGER_DOMAIN=1234-42-42-42-42.ngrok-free.app
```

4. Quit the start script and launch it again, or run this:

```bash
./stop.sh && ./start.sh
```

### Registry setup

If you want to deploy v3 projects, you will need access to a Docker registry. The [CLI deploy](/cli-deploy) command will push the images, and then the worker machine can pull them when needed. We will use Docker Hub as an example.

1. Sign up for a free account at [Docker Hub](https://hub.docker.com/)

2. Edit the `.env` file and add the registry details

```bash
DEPLOY_REGISTRY_HOST=docker.io
DEPLOY_REGISTRY_NAMESPACE=<your_dockerhub_username>
```

3. Log in to Docker Hub both locally and your server. For the split setup, this will be the worker machine. You may want to create an [access token](https://hub.docker.com/settings/security) for this.

```bash
docker login -u <your_dockerhub_username>
```

4. Restart the services

```bash
./stop.sh && ./start.sh
```

5. You can now deploy v3 projects using the CLI with these flags:

```bash
npx trigger.dev@latest deploy --self-hosted --push
```

## Part 2: Split services

With this setup, the webapp will run on a different machine than the worker components. This allows independent scaling of your workload capacity.

### Webapp setup

All steps are the same as in Part 1, except for the following:

1. Run the start script with the `webapp` argument

```bash
./start.sh webapp
```

2. Tunnelling is now _required_. Please follow the tunnelling section from above.

### Worker setup

1. Copy your `.env` file from the webapp to the worker machine

```bash
# an example using scp
scp -3 root@<webapp_machine>:docker/.env root@<worker_machine>:docker/.env
```

2. Run the start script with the `worker` argument

```bash
./start.sh worker
```

2. Tunnelling is _not_ required for the worker components.

## Checkpoint support

<Warning>
  This requires an _experimental Docker feature_. Successfully checkpointing a task today, does not
  mean you will be able to restore it tomorrow. Your data may be lost. You've been warned!
</Warning>

Checkpointing allows you to save the state of a running container to disk and restore it later. This can be useful for
long-running tasks that need to be paused and resumed without losing state. Think fan-out and fan-in, or long waits in email campaigns.

The checkpoints will be pushed to the same registry as the deployed images. Please see the [Registry setup](#registry-setup) section for more information.

### Requirements

- Debian, **NOT** a derivative like Ubuntu
- Additional storage space for the checkpointed containers

### Setup

Underneath the hood this uses Checkpoint and Restore in Userspace, or [CRIU](https://github.com/checkpoint-restore/criu) in short. We'll have to do a few things to get this working:

1. Install CRIU

```bash
sudo apt-get update
sudo apt-get install criu
```

2. Tweak the config so we can successfully checkpoint our workloads

```bash
mkdir -p /etc/criu

cat << EOF >/etc/criu/runc.conf
tcp-close
EOF
```

3. Make sure everything works

```bash
sudo criu check
```

3. Enable Docker experimental features, by adding the following to `/etc/docker/daemon.json`

```json
{
  "experimental": true
}
```

4. Restart the Docker daemon

```bash
sudo systemctl restart docker
```

5. Uncomment `FORCE_CHECKPOINT_SIMULATION=0` in your `.env` file. Alternatively, run this:

```bash
echo "FORCE_CHECKPOINT_SIMULATION=0" >> .env
```

6. Restart the services

```bash
# if you're running everything on the same machine
./stop.sh && ./start.sh

# if you're running the worker on a different machine
./stop.sh worker && ./start.sh worker
```

## Telemetry

By default, the Trigger.dev webapp sends telemetry data to our servers. This data is used to improve the product and is not shared with third parties. If you would like to opt-out of this, you can set the `TRIGGER_TELEMETRY_DISABLED` environment variable in your `.env` file. The value doesn't matter, it just can't be empty. For example:

```bash
TRIGGER_TELEMETRY_DISABLED=1
```

## Login via the CLI

To avoid being redirected to the Cloud login page when using the CLI, you can specify the URL of your self-hosted instance with the `-a` flag. For example:

```
npx trigger.dev@latest login -a http://example.com
```
